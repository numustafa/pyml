{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start working on the exercise\n",
    "\n",
    "- Use Python version 3.7 up to 3.9. Make sure not to use Python 3.10\n",
    "- It is highly recommended to create a virtual environment for this course. You can find resources on how to create a virtual environment on the ISIS page of the course.\n",
    "- Make sure that no assertions fail or exceptions occur, otherwise points will be subtracted.\n",
    "- Use all the variables given to a function unless explicitly stated otherwise. If you are not using a variable you are doing something wrong.\n",
    "- Read the **whole** task description before starting with your solution.\n",
    "- After you submit the notebook more tests will be run on your code. The fact that no assertions fail on your computer locally does not guarantee that you completed the exercise correctly.\n",
    "- Please submit only the notebook file with its original name. If you do not submit an `ipynb` file you will fail the exercise.\n",
    "- Edit only between YOUR CODE HERE and END YOUR CODE.\n",
    "- Verify that no syntax errors are present in the file.\n",
    "- Before uploading your submission, make sure everything runs as expected. First, restart the kernel (in the menubar, select Kernel\\Restart) and then run all cells (in the menubar, select Cell\\Run All)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if (3,7) <= sys.version_info[:2] <= (3, 9):\n",
    "    print(\"Correct Python version\")\n",
    "else:\n",
    "    print(f\"You are using a wrong version of Python: {'.'.join(map(str,sys.version_info[:3]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ee63ebe94909c1120d2c0316d4edb95",
     "grade": false,
     "grade_id": "cell-22993fe6125aa495",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "$$\\Large\\textbf{Python Programming for Machine Learning}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\Large\\textbf{Exam}$$\n",
    "<hr>\n",
    "\n",
    "<img src='tu_logo.png' width='200'>\n",
    "<br>\n",
    "\n",
    "$$\\text{Department of Intelligent Data Analysis and Machine Learning}$$\n",
    "\n",
    "<br>\n",
    "$${28\\text{th of November}\\> 2022}$$\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Read before starting with the exam !\n",
    "\n",
    "The exam has a similar format to the exercise sheets you completed throught the course.\n",
    "\n",
    "Each exercise consists of: \n",
    "- Explanation \n",
    "- Implementation\n",
    "- Overwrite\n",
    "\n",
    "The overwrite part means that after your function has been tested the expected value will be placed in the corresponding variables, so that if you get stuck you can continue with the next exercise.\n",
    "\n",
    "For each exercise there will be a maximum number of loops allowed. If your function contains more loops than allowed, you will be notified during the function definition, and the function will automatically fail in the hidden tests. Note that \"unrolling a loop\" (repeating a line many times) is also considered a loop.\n",
    "\n",
    "For technical reasons the following functions are **banned** throughout the notebook.\n",
    "\n",
    "- map\n",
    "- sum (but np.sum is allowed)\n",
    "- filter\n",
    "- np.vectorize\n",
    "- np.fromiter\n",
    "- np.fromfunction\n",
    "- np.apply_along_axis\n",
    "\n",
    "If you use one of these functions in your solution will **not** get any points.\n",
    "\n",
    "**Important:** \n",
    "   \n",
    "- Execute every cell in the notebook. You may also try to restart your kernel and execute all cells, in case something went wrong.\n",
    "\n",
    "- If you were not able to implement one function you may proceed with a next execise by using data generated from the _expected_ output functions.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "54e5690ee36a29007da55522a19ae009",
     "grade": false,
     "grade_id": "cell-c19daf40b3550f45",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Personal student information\n",
    "\n",
    "In the following cell fill in your **real** personal information. Make sure that the code compiles. This information may be used later for you class certificates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Max\"  # your first name\n",
    "MID_NAME = \"\"  # your middle or empty string ''\n",
    "SURNAME = \"Mustermann\"  # your last name\n",
    "\n",
    "MATRICULATION_NUMBER = -1  # e.g. 412342 as integer\n",
    "\n",
    "HOME_UNIVERSITY = \"\"  # e.g. TU Berlin, HU Berlin, Uni Potsdam, etc...\n",
    "MODULE_NAME = \"\"  # e.g CA, ML-1, ML-2, Standalone\n",
    "COURSE_OF_STUDY = \"\"  # e.g. Mathematics, Computer Sciences, Physic, etc...\n",
    "DEGREE = \"\"  # e.g. Erasmus, Bachelor, Diplom, Master, PhD or Guest (all others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown as md\n",
    "\n",
    "md(\n",
    "    f\"## Hello {NAME} {MID_NAME} {SURNAME} \\n\"\n",
    "    f\"### Your matriculation number is {MATRICULATION_NUMBER} \\n\"\n",
    "    f\"### You study at {HOME_UNIVERSITY} {COURSE_OF_STUDY} {DEGREE} \\n\"\n",
    "    f\"### Module name: {MODULE_NAME}\\n\"\n",
    "    \"## [zoom exam room](https://tu-berlin.zoom.us/j/67524239950?pwd=WThJNHZCT3pJbTdIVEZ3L0xKZ0s3Zz09)\\n\"\n",
    "    \"## password: 068961\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "HTML(\n",
    "    \"\"\"\n",
    "<style>\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    margin:auto;\n",
    "}\n",
    ".prompt \n",
    "    display:none;\n",
    "}\n",
    "</style>\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b6073505e8583352350252c8a85e23c",
     "grade": false,
     "grade_id": "cell-e29eed866958407e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "import minified\n",
    "from minified import max_allowed_loops\n",
    "overwrite = minified.overwrite()\n",
    "\n",
    "from unittest import TestCase\n",
    "t = TestCase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bdfd5033b386091eba4b04db336fb71e",
     "grade": true,
     "grade_id": "cell-65d688a4a676d6f2",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "660330e037b94d4ac3f4169a26786922",
     "grade": false,
     "grade_id": "cell-b000ced9c864565a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this exam you want to write an algorithm that finds a function $f(x)$ that closely represents two dimensional data.\n",
    "\n",
    "* You do not need to understand how this algorithm works to complete these tasks. It is sufficient to read the bullet points to know what you _have to_ implement. The longer task descriptions explain what is happening.\n",
    "\n",
    "* There are no loops allowed in _any_ of the excercises.\n",
    "\n",
    "# $\\textbf{Exercise 1: } \\text{Creating the dataset (44 Points})$\n",
    " \n",
    "In the first excercise you want to generate two dimensional data on which you can train and test your algorithm. You will first generate the x- and y-coordinates, split your data into train and test sets, and plot both of them.\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Exercise 1.1: Generate x-coordinates of the datapoints ( 6 points ) \n",
    "\n",
    "<hr>\n",
    "\n",
    "First you will generate the x-coordinates using a uniform distribution. Since the y-coordinates will follow a specific pattern, their order will be relevant. Therefore you will need to return the x-coordinates in ascending order.\n",
    "\n",
    "* Draw $N=500$ samples from the uniform distribution in the range $[ -4\\pi, 2\\pi )$.\n",
    "\n",
    "$$\\Large{\n",
    "x \\sim \\mathcal{U}niform[-4\\pi, 2\\pi),\\quad x \\in \\mathbb{R}^{N}\n",
    "}$$\n",
    "\n",
    "* Return $x$ sorted is ascending order.\n",
    "\n",
    "\n",
    "* Loops allowed in this excercise: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "36b4bab1b77e9c232b877c6a3e0e18d6",
     "grade": false,
     "grade_id": "cell-617f884b4a7561e6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@max_allowed_loops(0)\n",
    "def generate_x(n_samples: int) -> np.ndarray:\n",
    "    '''\n",
    "    Create a np.ndarray vector containing `n_samples` sorted values drawn \n",
    "    from a uniform distribution in [-4π, 2π).\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError(\"Replace this line with your code\")\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f1262ebe90746231a967492bbe48bffa",
     "grade": true,
     "grade_id": "cell-bfbb0a01b7679ebf",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "n_samples = 500\n",
    "\n",
    "x = generate_x(n_samples)\n",
    "print(x[[0,1,2,3,4,-5,-4,-3,-2,-1]])\n",
    "# running function again should yield different results\n",
    "x2_size = n_samples + 10\n",
    "x2 = generate_x(x2_size)\n",
    "\n",
    "def check_x(x, size):\n",
    "    assert x.shape == (size,)\n",
    "    t.assertTrue(np.all(x >= - 4 * np.pi))\n",
    "    t.assertTrue(np.all(x <= 2 * np.pi))\n",
    "    np.testing.assert_array_equal(np.diff(x) >= 0, True, 'output is not sorted')\n",
    "    \n",
    "\n",
    "\n",
    "assert (x != x2[:n_samples]).mean() > 0.95\n",
    "check_x(x, n_samples)\n",
    "check_x(x2, x2_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1f160ca38486faccd83fd32b8518d23",
     "grade": false,
     "grade_id": "cell-f57a387ce173a739",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x = overwrite.ex1()\n",
    "x.shape, x[:10], x[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4b58110fab5c200135f5c154ae714f45",
     "grade": false,
     "grade_id": "cell-d5ec24a6184b5dba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 1.2: Generate y-values of the datapoints ( 10 points ) \n",
    "\n",
    "<hr>\n",
    "\n",
    "Since you want to simulate the behavior of the data that has a true function $f(x)$ to represent it, you need to define that function $f(x)$. Even though you will later want your algorithm to find a polynomial function to represent the data, the function you will use to generate it will not be polynomial. That means that there is not one perfect function the algorithm can find, as there would also not be in most real scenarios. Instead you will use sine and cosine functions to generate your data. Since in most real data there is noise, you will need to add normal (Gaussian) noise $\\mathcal{E}$ to your data.\n",
    "\n",
    "In this excercise you want to calculate a y-coordinate for each x-coordinate of your toy dataset.\n",
    "\n",
    "* Define the actual function $f(x)$ to generate the y-coordinates: \n",
    "\n",
    "$$\\Large{f(x) = \\sin(x) + \\cos(\\frac x 2)}$$\n",
    "\n",
    "* Generate some normal (Gaussian) noise $\\mathcal{E}$:\n",
    "\n",
    "\n",
    "$$\\Large{\n",
    "\\quad \\mathcal{E} \\sim \\mathcal{N}(0, \\sigma=0.5),\\quad \\mathcal{E} \\in \\mathbb{R}^{N}\n",
    "}$$\n",
    "\n",
    "The noise can be generated using a numpy function. The standard deviation of the noise should be 0.5.\n",
    "\n",
    "* Calculate the y-coordinates by adding the normal Gaussian noise to $f(x)$:\n",
    "\n",
    "$$\\Large{y = f(x) + \\mathcal{E}}$$\n",
    "\n",
    "\n",
    "* Loops allowed in this excercise: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c025c36540d9c0c18848f2917e2f7669",
     "grade": false,
     "grade_id": "cell-5b215bb4d4fbb87c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, Callable, TypeVar\n",
    "\n",
    "@max_allowed_loops(0)\n",
    "def generate_y(\n",
    "    x: np.ndarray, std: float = 0.5\n",
    ") -> Tuple[Callable[[np.ndarray], np.ndarray], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Create an output for each sample created in the previous exercise.\n",
    "    The underlying signal is `sin(x) + cos(x / 2)`. A normal noise with\n",
    "    a standard deviation as provided in the argument `std` (the mean of the\n",
    "    noise is 0) is added to the signal. The output of the function is a\n",
    "    callable (a function) that outputs \"clean\" values (without noise) from\n",
    "    the underlying signal, and the calculated noisy values calculated for \n",
    "    each sample in `x`.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError(\"Replace this line with your code\")\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return f, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "be7d27e1112f3b98c8d1286236ee3887",
     "grade": true,
     "grade_id": "cell-9b8982571a1a07dd",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "f, y = generate_y(x, std=0.5)\n",
    "\n",
    "assert callable(f), \"f should behave like a function.\"\n",
    "x_test = np.arange(3) * np.pi\n",
    "np.testing.assert_allclose(f(x_test), [1, 0, -1], atol=1e-5)\n",
    "\n",
    "\n",
    "def test_y(x, f, y, std):\n",
    "    assert y.shape == x.shape\n",
    "    clean_signal_difference = f(x) - y\n",
    "    np.testing.assert_allclose(np.std(clean_signal_difference), std, atol=1e-1)\n",
    "\n",
    "\n",
    "test_y(x, f, y, 0.5)\n",
    "\n",
    "x2 = np.arange(10000)\n",
    "std2 = 10\n",
    "f2, y2 = generate_y(x2, std=std2)\n",
    "test_y(x2, f2, y2, std2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "633ae5f4545b44210ae84f726ec52161",
     "grade": false,
     "grade_id": "cell-0db80b8ad3fed20c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "f, y = overwrite.ex2()\n",
    "\n",
    "f(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b1fecff84a021fe5b503ffa1881608de",
     "grade": false,
     "grade_id": "cell-8461ec976651de9f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 1.3: Split the data into training and test sets ( 10 points ) \n",
    "\n",
    "<hr>\n",
    "\n",
    "You will now need to split up the data into a training and a test set. \n",
    "\n",
    "* The train set should contain 80% by default of the data while the test set contains the remaining datapoints.\n",
    "\n",
    "* Both the train and the test values have to be sorted, in the same way the given dataset is sorted (ascending x-values).\n",
    "\n",
    "* Return four numpy arrays: The x values of the training set, the y values of the training set, the x values of the test set and the y values of the test set.\n",
    "\n",
    "\n",
    "* Loops allowed in this excercise: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "73ecfd9a33d38010939af2c4a1921c81",
     "grade": false,
     "grade_id": "cell-467cd0108b223d67",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "@max_allowed_loops(0)\n",
    "def split_data_and_sort(\n",
    "    x: np.ndarray, y: np.ndarray, sp_ratio: float = 0.8\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray,]:\n",
    "    '''\n",
    "    Given x & y arrays and a split ratio, split the data in x & y\n",
    "    into training and test arrays, such that the ratio of the length of the\n",
    "    train arrays and the original array is as close to `sp_ratio` as possible.\n",
    "    \n",
    "    The returned train and test arrays are sorted again such that the x arrays\n",
    "    are in ascending order, and the association between x and y is kept.\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError(\"Replace this line with your code\")\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "\n",
    "    return x_tr, y_tr, x_te, y_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c49c770bd741104a453ead1cf06994e5",
     "grade": true,
     "grade_id": "cell-2c61711b9da03e23",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sp_ratio = 0.8\n",
    "x_tr, y_tr, x_te, y_te = split_data_and_sort(x, y, sp_ratio)\n",
    "\n",
    "assert np.all(np.diff(x_tr) >= 0), 'x_tr is not sorted'\n",
    "assert np.all(np.diff(x_te) >= 0), 'x_te is not sorted'\n",
    "\n",
    "assert len(x_tr) + len(x_te) == len(x)\n",
    "assert len(y_tr) + len(y_te) == len(y)\n",
    "\n",
    "assert len(x_tr) == int(sp_ratio * len(x))\n",
    "assert len(y_tr) == int(sp_ratio * len(y))\n",
    "\n",
    "assert len(x_te) == int((1 - sp_ratio) * len(x)) + 1\n",
    "assert len(y_te) == int((1 - sp_ratio) * len(y)) + 1\n",
    "\n",
    "\n",
    "x_tr2, *_ = split_data_and_sort(x, y, sp_ratio)\n",
    "assert np.any(x_tr2 != x_tr)\n",
    "\n",
    "x_tr3, _, x_te3, _ = split_data_and_sort(x, y, 0.5)\n",
    "assert len(x_tr3) == len(x_te3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b3197fa8f04224da6b2fc7e7ad9a56ff",
     "grade": false,
     "grade_id": "cell-c7548e2df61dcfbc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x_tr, y_tr, x_te, y_te = overwrite.ex3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f2ef114473e4cce771f414ff96030c3f",
     "grade": false,
     "grade_id": "cell-b3ca95ce4c406c53",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 1.4: Plot the data ( 18 points ) \n",
    "\n",
    "<hr> \n",
    "\n",
    "Now lets take a closer look at the data you have generated. You will want to plot both the function $f(x)$ the data represents, as well as the toy data itself. \n",
    "\n",
    "* Plot the data in a scatter plot with 50% transparency.\n",
    "\n",
    "* Plot the function $f(x)$ in a dashed, red line with a linewidth of 4.\n",
    "\n",
    "* The title for the plot should be dynamic, as it is passed to the function. Set the fontsize of the title to 17.\n",
    "\n",
    "* Label both the scatter plot as 'data' and the line plot as '$f(x)$'.\n",
    "\n",
    "* Set the labels of the axes to a fontsize of 14. Those labels are also passed to the plotting function.\n",
    "\n",
    "* The label of the y-axis should be rotated 90 degrees.\n",
    "\n",
    "\n",
    "* Loops allowed in this excercise: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25de7a483dff76f0f8862910d3d71903",
     "grade": true,
     "grade_id": "cell-e55b74e86c4a34f1",
     "locked": false,
     "points": 18,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from typing import Callable, Optional\n",
    "\n",
    "@max_allowed_loops(0)\n",
    "def plot_data(\n",
    "    x: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    f: Callable[[np.ndarray], np.ndarray],\n",
    "    y_hat: Optional[np.ndarray] = None,\n",
    "    std: float = 0,\n",
    "    title: str = \"\",\n",
    "    ax=None,\n",
    "    ylabel: str = \"y_values\",\n",
    "    xlabel: str = \"x_values\",\n",
    "    show_legend: bool = False,\n",
    "):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, figsize=(12, 5))\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError(\"Replace this line with your code\")\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "\n",
    "    if y_hat is not None:\n",
    "        ax.plot(x, y_hat, \"--\", color=\"g\", linewidth=3, alpha=0.8, label=\"$\\hat{f}(x)$\")\n",
    "        ax.fill_between(x, y_hat + std, y_hat - std, color=\"g\", alpha=0.1)\n",
    "\n",
    "    if show_legend:\n",
    "        ax.legend(fontsize=16)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3dc125b598afac38cbfa9c8993add72f",
     "grade": false,
     "grade_id": "cell-5618c6f4be2c6f55",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plot_data(x_tr, y_tr, f, title='Scatter plot of the train data', show_legend=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08efff7eb8bb6ab0a99045506517d52e",
     "grade": false,
     "grade_id": "cell-6c79e0d805325070",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plot_data(x_te, y_te, f, title='Scatter plot of the test data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5c1ff70c8699e5a8af7e950573d2225f",
     "grade": false,
     "grade_id": "cell-8d5d56c7d9c723a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 1.5: Expected output ( 0 points )\n",
    "\n",
    "<hr>\n",
    "\n",
    "If you have done all above tasks correctly, the plots should look like this:\n",
    "\n",
    "* Compare your plots to the correct ones.\n",
    "\n",
    "* This gives you a chance to correct obvious mistakes but do not let yourself get stuck here.\n",
    "\n",
    "* There is no code to write in this excercise.\n",
    "\n",
    "<img src='./images/scatter_train.png' width=500>\n",
    "<img src='./images/scatter_test.png' width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69454709bed9d7256ffde825717b70b8",
     "grade": true,
     "grade_id": "cell-85eb2dec100f3da3",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "05685a21d3920d39fd3b91eb6d411dba",
     "grade": false,
     "grade_id": "cell-d91f928ad02d06dc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# $\\textbf{Exercise 2: } \\text{Linear Regression (29 Points})$\n",
    " \n",
    "In the second excercise you will write the Linear Regression algorithm to predict the function $\\hat{f}(x)$ that fits the data best. The polynomial degree of a function is the greatest exponent of that function. E.g. a function with a polynomial degree of 3 looks like this: $$\\hat{f_3}(x) = a + b x^1 + c x^2 + d x^3\\quad \\mathcal{a,b,c,d} \\in \\mathbb{R}$$\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Exercise 2.1: Calculate the polynomial features of $x$ ( 7 points ) \n",
    "\n",
    "<hr>\n",
    "\n",
    "What you want to do first is to calculate $X$ containing the x-values to the power of $0, 1, 2, ...$ up to to the power of $\\text{degree}$. These values are later used to find the parameters they need to be multiplied with to compute $\\hat{f}(x)$. This `get_X(x, degree)` needs to be adaptable to a varying degree, as sometimes a function with a polynomial degree of 6 fits better and other times a function with a polynomial degree of 2 is sufficient.\n",
    "\n",
    "* Compute $X$.\n",
    "\n",
    "$$d = \\text{degree + 1}$$\n",
    "\n",
    "$$\\Large{\n",
    "X = [x^0, x^1, ...,x^d] \\in \\mathbb{R}^{(N,d)}, \\text{where} \\quad x \\in \\mathbb{R}^{(N,1)}\n",
    "}$$\n",
    "\n",
    "* `get_X(x, degree)` needs to work dynamically with every degree it is getting passed.\n",
    "\n",
    "\n",
    "\n",
    "* Loops allowed in this excercise: 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a6b5b8a26e679b034b8d9421ae1515bd",
     "grade": false,
     "grade_id": "cell-9fda0217e573b34a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@max_allowed_loops(0)\n",
    "def get_X(x: np.ndarray, degree: int) -> np.ndarray:\n",
    "    '''\n",
    "    Calculate all the powers of every input x up to and including `degree`.\n",
    "    If the input is x=[1,2,3,10], degree=2 the output is:\n",
    "    [\n",
    "        [1, 1, 1],\n",
    "        [1, 2, 4],\n",
    "        [1, 3, 9],\n",
    "        [1, 10, 100],\n",
    "    ]\n",
    "    '''\n",
    "    assert degree >= 0\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError(\"Replace this line with your code\")\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ac0bae37ae3892875173248aaac63b71",
     "grade": true,
     "grade_id": "cell-2cb2643c1644da53",
     "locked": true,
     "points": 7,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "degree = 3\n",
    "X_tr = get_X(x_tr, degree)\n",
    "print(X_tr.shape)\n",
    "\n",
    "assert X_tr.shape == (400, 4)\n",
    "\n",
    "np.testing.assert_equal(X_tr[:,0], 1)\n",
    "\n",
    "for i in range(X_tr.shape[1] - 1):\n",
    "    np.testing.assert_allclose(X_tr[:,i+1] / X_tr[:, i], x_tr)\n",
    "\n",
    "    \n",
    "assert get_X(np.random.rand(10), 20).shape == (10,21)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80b428094cf8fe4e33112c9c4115058e",
     "grade": true,
     "grade_id": "cell-8ea8fc4e399f902a",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_tr = overwrite.ex6()\n",
    "X_tr.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b5213a67d85ef29e98eb7de5b90835ff",
     "grade": false,
     "grade_id": "cell-187de6da672746e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 2.2: Find the W containing the parameters ( 10 points ) \n",
    "\n",
    "<hr>\n",
    "\n",
    "Next you need to find the parameters of your polynomial function. In the example $\\hat{f_3}(x) = a + b x^1 + c x^2 + d x^3\\quad \\mathcal{a,b,c,d} \\in \\mathbb{R}$ you would want to find $\\mathcal{a,b,c,d}$ so that $\\hat{f_3}(x)$ fits your data best. To solve this problem you need $y$ to find a vector $W$ so that $\\hat{f}(x) = X W$. The value of $\\lambda$ regularizes the variance of the model parameters. \n",
    "\n",
    "* Calculate W:\n",
    "\n",
    "$$\\Large{\n",
    "W = (X^{\\top}X + \\lambda I_d)^{-1}X^{\\top}y}, \\quad \\text{where}\\quad X\\in \\mathbb{R}^{(N,d)}, \\quad I_d \\in \\{0,1\\}^{(d,d)}\n",
    "$$\n",
    "\n",
    "$$\\text{Such that}\\quad \\forall A : \\quad A I = A$$\n",
    "\n",
    "$$\\text{E.g. }d=3, \\quad I_3 = \\begin{pmatrix}1 & 0 & 0\\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1  \\end{pmatrix}\n",
    "$$ \n",
    "\n",
    "\n",
    "* Loops allowed in this excercise: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7990069b4556de4d7d73af1d46b8c915",
     "grade": false,
     "grade_id": "cell-e2902b6c80147664",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@max_allowed_loops(0)\n",
    "def calc_W(X: np.ndarray, y: np.ndarray, lambd: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the W vector that fits the data `X` into labels `y`, while\n",
    "    using the regularization parameter `lambd`\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError(\"Replace this line with your code\")\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a6a8eeacbc36953a6636408bda6db3b",
     "grade": true,
     "grade_id": "cell-9db90a972a1df2d8",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(X_tr.shape)\n",
    "\n",
    "W_tr = calc_W(X_tr, y_tr, 1)\n",
    "W_tr.shape == (4,)\n",
    "assert 0.109 in W_tr.round(3)\n",
    "\n",
    "calc_W(np.random.rand(20,7), np.random.rand(20), 0).shape == (7,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c0671de3c6c417a238dffa0aae56d149",
     "grade": true,
     "grade_id": "cell-742472dbd26c43e3",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "W_tr = overwrite.ex7()\n",
    "W_tr.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e213bec20c8b6d1c6efe11338432345",
     "grade": false,
     "grade_id": "cell-054ee88d1899a9c2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 2.3: Compute predictions for the train data ( 2 points ) \n",
    "\n",
    "<hr> \n",
    "\n",
    "Now that you have both the polynomial features $X$ and the parameters $W$ of your function $\\hat{f}(x)$ you can compute your prediction of $\\hat{y} = \\hat{f}(x)$ for any x-value.\n",
    "\n",
    "* Compute the predictions for the training data:\n",
    "\n",
    "$$\\Large{\\hat{y} = X W}$$\n",
    "\n",
    "\n",
    "* Loops allowed for this exercise: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f3b09701b62e373d8a8d62e01942bb08",
     "grade": false,
     "grade_id": "cell-7f30b2f14679c001",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@max_allowed_loops(0)\n",
    "def compute_y_hat(X: np.ndarray, W: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Evaluate the fitted polynomial encoded in W for all values in X.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError(\"Replace this line with your code\")\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     10
    ],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0e7112ca485a97d4ca672e362532b46",
     "grade": true,
     "grade_id": "cell-933d78f36ae5c21a",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "y_hat_tr = compute_y_hat(X_tr, W_tr)\n",
    "\n",
    "assert len(y_hat_tr.shape) == 1\n",
    "assert y_hat_tr.shape[0] == X_tr.shape[0]\n",
    "\n",
    "assert compute_y_hat(np.random.rand(20, 4), np.random.rand(4)).shape == (20,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b2bf103d9ca3b953d0e15407b677c19c",
     "grade": true,
     "grade_id": "cell-1628da5b04fa5767",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "y_hat_tr = overwrite.ex8()\n",
    "y_hat_tr.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b60a884e1d266aedbb63b1739e87b28",
     "grade": false,
     "grade_id": "cell-dbb8638165e99601",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 2.4: Linear Regression Class ( 5+5 Points )\n",
    "\n",
    "It is practical to organize the steps above in one Linear Regression class.\n",
    "\n",
    "* Define the $\\text{LinRegression}$ class and its $\\text{train}$ and $\\text{predict}$ methods using the functions implemented above.\n",
    "* The train function computes and stores $W$. To compute $W$ you will also have to compute $X$.\n",
    "* The predict function computes $\\hat{y}$ by first computing $X$ and using the $W$ calculated in the train function.\n",
    "* If $W$ hasn't been calculated yet, the model is not trained. Throw a `ModelNotTrainedException` in that case.\n",
    "\n",
    "\n",
    "* Loops allowed for this exercise: 0\n",
    "\n",
    "\n",
    "_Note_: Even if you haven't been able to implement the functions required, impelement the \n",
    "class as if they were available to you. When we check your code we will overwrite the functions with reference implementations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0430321cfea4ac087a632ab8554ab008",
     "grade": false,
     "grade_id": "cell-b84f9d24bd4fc850",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class ModelNotTrainedException(Exception):\n",
    "    '''\n",
    "    Exception to represent when a model has not \n",
    "    been trained but it is being used.\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "848f6aee05321eca2c3c39e1ea881553",
     "grade": false,
     "grade_id": "cell-8dc283775ce960d2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class LinRegression(object):\n",
    "    def __init__(self, degree: int, lambd: float = 1) -> None:\n",
    "        self.W = None\n",
    "        self.degree = degree\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def __eq__(self, other: \"LinRegression\") -> bool:\n",
    "        return (\n",
    "            self.degree == other.degree\n",
    "            and self.lambd == other.lambd\n",
    "            and np.allclose(self.W, other.W)\n",
    "        )\n",
    "\n",
    "    def train(self, x: np.ndarray, y: np.ndarray) -> \"LinRegression\":\n",
    "        \"\"\"\n",
    "        Train the linear regression instance, by calculating the\n",
    "        polynomial features for all elements in `x` using `get_X`,\n",
    "        calculating the W polynomial using `calc_W` and then\n",
    "        storing the W vector in the instance member `self.W`.\n",
    "\n",
    "        The method returns the calling instance.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError(\"Replace this line with your code\")\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, x) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict the y value for all inputs in x. First the\n",
    "        polynomial features of all elements in `x` are calculated\n",
    "        using `get_X`, and then the predictions using `compute_y_hat`.\n",
    "\n",
    "        If the instance has not been trained, an `ModelNotTrainedException`\n",
    "        is raised.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError(\"Replace this line with your code\")\n",
    "        # YOUR CODE HERE\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2bb48b0ad7971c780e3d631eb2c98b86",
     "grade": true,
     "grade_id": "cell-7b16f7d066bb2e8b",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "degree = 3\n",
    "lambd = 0.1\n",
    "linReg = LinRegression(degree, lambd)\n",
    "assert linReg.W is None\n",
    "linRegTrained = linReg.train(x_tr, y_tr)\n",
    "assert linRegTrained is linReg\n",
    "assert linReg.W is not None\n",
    "\n",
    "LinRegression(4, 0).train(\n",
    "    np.random.rand(20), np.random.rand(20)\n",
    ").W.shape == (5,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "017b8c1bbc887f243f6245e8535d9ffd",
     "grade": true,
     "grade_id": "cell-a7f29930803f54fe",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "linReg = LinRegression(degree, lambd).train(x_tr, y_tr)\n",
    "\n",
    "y_hat_te = linReg.predict(x_te)\n",
    "\n",
    "try:\n",
    "    linReg = LinRegression(degree, lambd).predict(np.random.rand(10))\n",
    "except ModelNotTrainedException as error:\n",
    "    \"Your code should throw an exception and get here.\"\n",
    "else:\n",
    "    assert False, \"Your function should raise a ModelNotTrainedException.\"\n",
    "\n",
    "y_hat_te\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7287dc70f427ae927d9aa216867f5adb",
     "grade": true,
     "grade_id": "cell-bc7502d1f5f76084",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "y_hat_te = overwrite.ex10()\n",
    "y_hat_te.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\textbf{Exercise 3: } \\text{Parameter variation (27 Points})$\n",
    " \n",
    "In the third excercise you will use everything you have implemented so far to see how good the predictions of $\\hat{f}(x)$ are. You will also check what influence varying polynomial degrees and varying lambda values have on the predictions to find the best parameter setting for this dataset.\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Exercise 3.1:  Define the objective function ( 3 points ) \n",
    "<hr>\n",
    "\n",
    "The objective function of Linear Regression is to minimize the sum of the squares of the difference between $y$ and $\\hat{y}$ so that we can find the function $\\hat{f}(x)$ that is closest to the true model.\n",
    "\n",
    "* Write the objective function:\n",
    "\n",
    "$$\\Large{\\mathcal{L}_{te} = \\frac1 2|| y_{te} - \\hat{y}_{te} ||_2 = \\frac1 2 \\sqrt{\\sum_{i}(y^{(i)}_{te} - \\hat{y}^{(i)}_{te})^2}}\n",
    "$$\n",
    "\n",
    "\n",
    "* Loops allowed in this exercise: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8aaeed67e984642810ff88f650c0df1c",
     "grade": false,
     "grade_id": "cell-370fa60325a7aa6a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@max_allowed_loops(0)\n",
    "def L(y: np.ndarray, y_hat: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the L2 loss between the predicted and generated values.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError(\"Replace this line with your code\")\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e776e6eed34d10e9155a69b2f0102a1",
     "grade": true,
     "grade_id": "cell-f5784bfd6c9c59e4",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "L_te = L(y_te, y_hat_te)\n",
    "assert 4.2 < L_te < 4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c6b737f6048eb758c1717c0cc05ceaa8",
     "grade": true,
     "grade_id": "cell-581318faa338e8ba",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c5ade235506014c2618ecbba02420eae",
     "grade": false,
     "grade_id": "cell-7a0e7612845a8d95",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 3.2: Compute the standard deviation ( 4 Points )\n",
    "\n",
    "The standard deviation is an interesting measurement to see how different $y$ and $\\hat{y}$ are from each other.\n",
    "\n",
    "* Compute the standard deviation:\n",
    "\n",
    "$$\\Large{\n",
    "\\sigma = \\sqrt{\\frac1 n \\sum_{i=1}^N(y - \\hat{y})^2}\n",
    "}$$\n",
    "* Solve this exercise without using `np.std`.\n",
    "\n",
    "\n",
    "* Loops allowed for this exercise: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ddb09f5fe980ff122e469b55fa34e4e2",
     "grade": false,
     "grade_id": "cell-8a46e95a9f659fdb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@max_allowed_loops(0)\n",
    "def std_(y: np.ndarray, y_hat: np.ndarray) -> float:\n",
    "    '''\n",
    "    Calculate the standard deviation of the differences between\n",
    "    the generated and predicted values.\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError(\"Replace this line with your code\")\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b9ffd64129399be6c0a942f5818bb6de",
     "grade": true,
     "grade_id": "cell-c1ef164b9b2e8007",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_allclose(\n",
    "    np.std(y_tr - y_hat_tr),\n",
    "    std_(y_tr, y_hat_tr),\n",
    "    atol=1e-4,\n",
    ")\n",
    "\n",
    "print(std_(y_tr, y_hat_tr))\n",
    "assert 0.79 < std_(y_tr, y_hat_tr) < 0.80\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c43db55846ec524bd39f55487c2c205",
     "grade": true,
     "grade_id": "cell-da537aa333b87e15",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a58e22bfdd8d66b2b2bf0f944bc9ab3a",
     "grade": false,
     "grade_id": "cell-eb7e130eaf2a91d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 3.3:  Plotting the test data with different settings ( 15 points )\n",
    "\n",
    "<hr>\n",
    "\n",
    "With the test data you can now visualize the impact different degrees and lambdas have on the computed function $\\hat{f}(x)$.\n",
    "\n",
    "* Train a linear regression model for each combination of lambdas and degrees __on the training data__. (The `for` loop is already implemented for you.)\n",
    "* Predict $\\hat{y}$ __for the test data__.\n",
    "* Calculate the L2 loss (using the `L` function) between $y$ and $\\hat{y}$.\n",
    "* Call the function `plot_data` from Excercise 1.4 and pass `x_te, y_te, f, y_hat, std, title, ax, ylabel, xlabel` and `show_legend` as arguments.\n",
    "\n",
    "\n",
    "The settings for every subplot that are passed to `plot_data` are:\n",
    "* Calculate the std of the test data.\n",
    "* The xlabel is dynamic: '$\\lambda$: {lambda}'\n",
    "* The ylabel is dynamic: 'degree: {degree}'\n",
    "* The title is dynamic: '$\\mathcal{L}_{te}: $ {L_te}'\n",
    "\n",
    "\n",
    "* Loops allowed in this exercise: 0\n",
    "\n",
    "Implementing this function requires a loop for iterating over the parameter combinations, but the part you have to fill in does not require any loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16986a836dbf73bf9cd9b98869f51dcc",
     "grade": true,
     "grade_id": "cell-eb464feb51de81f3",
     "locked": false,
     "points": 15,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from typing import Iterable, Tuple\n",
    "\n",
    "\n",
    "@max_allowed_loops(1)\n",
    "def plot_test_data_fitting(\n",
    "    degrees: Iterable[int],\n",
    "    lambdas: Iterable[float],\n",
    "    std: float = 0,\n",
    "    fs: Tuple[int, int] = (20, 20),\n",
    "    show_legend: bool = False,\n",
    "):\n",
    "\n",
    "    fig, axis = plt.subplots(\n",
    "        len(degrees), len(lambdas), sharex=True, sharey=True, figsize=fs, squeeze=False\n",
    "    )\n",
    "    for (n, degree), (k, lambd) in product(enumerate(degrees), enumerate(lambdas)):\n",
    "        ax = axis[n, k]\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError(\"Replace this line with your code\")\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c6077febd2c6cc5d52e657d39390e79b",
     "grade": false,
     "grade_id": "cell-a004c1390c0eaf72",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "degrees = [2, 6, 10, 16, 17]\n",
    "lambdas = [0, 1, 10]\n",
    "\n",
    "plot_test_data_fitting(degrees, lambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b5b80eabebf3d27ee16ac44faad4f40",
     "grade": false,
     "grade_id": "cell-id-1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 3.4: Expected output ( 0 points )\n",
    "\n",
    "<hr>\n",
    "\n",
    "If you have done all above tasks correctly, the plots should look like this:\n",
    "\n",
    "* Compare your plots to the correct ones.\n",
    "\n",
    "* This gives you a chance to correct obvious mistakes but do not let yourself get stuck here.\n",
    "\n",
    "* There is no code to write in this excercise.\n",
    "\n",
    "<img src='./images/fitting.png' width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f21f964803ca138ba5cf9394cc727df3",
     "grade": false,
     "grade_id": "cell-5b300d8182cdb1ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 3.5: Grid search of the parameters ( 5 points )\n",
    "\n",
    "<hr>\n",
    "\n",
    "Now that you have a way to rate how far $y$ is from $\\hat{y}$ (the objective function $L$), you can try different combinations of degrees and lambdas to find the ideal parameters.\n",
    "\n",
    "* Use the functions and classes you have implemented up to this point.\n",
    "* Train a linear regression model for each combination of lambdas and degrees __on the training data__.\n",
    "* Predict $\\hat{y}$ __for the test data__.\n",
    "* Calculate the L2 loss (using the `L` function) between $y$ and $\\hat{y}$.\n",
    "* Return the best loss corresponding model for the best combination of degree and lambda. \n",
    "\n",
    "\n",
    "* Number of loops allowed in this exercise: 0\n",
    "\n",
    "Implementing this function requires a loop for iterating over the parameter combinations, but the part you have to fill in does not require any loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = np.arange(2, 20)\n",
    "lambdas = [0, 1, 10, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1d9b0cc9de710ec3ec09ae566d7a65f7",
     "grade": false,
     "grade_id": "cell-ea5ab5efed640f92",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "@max_allowed_loops(1)\n",
    "def grid_search(\n",
    "    degrees: Iterable[int],\n",
    "    lambdas: Iterable[float],\n",
    "    x_tr: np.ndarray,\n",
    "    y_tr: np.ndarray,\n",
    "    x_te: np.ndarray,\n",
    "    y_te: np.ndarray,\n",
    ") -> Tuple[LinRegression, float]:\n",
    "    \"\"\"\n",
    "    Find the best hyperparameters for a linear regression model. For each\n",
    "    hyperparameter combination, train the model using the training data\n",
    "    and calculate the loss using the test data. If the trained model is\n",
    "    the best one trained so far according to the loss metric, store it.\n",
    "\n",
    "    Finally return the best linear regression model along with its test\n",
    "    loss.\n",
    "    \"\"\"\n",
    "\n",
    "    best_L = float(\"inf\")\n",
    "    best_linReg = None\n",
    "\n",
    "    for lambd, degree in product(lambdas, degrees):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError(\"Replace this line with your code\")\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "\n",
    "    return best_linReg, best_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "820e34e6668990a7d4bba0b20716fd90",
     "grade": true,
     "grade_id": "cell-f55023889d60923c",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "best_linReg, best_L = grid_search(degrees, lambdas, x_tr, y_tr, x_te, y_te)\n",
    "print(best_L)\n",
    "assert best_linReg.W is not None\n",
    "assert 2.5 < best_L < 2.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best degree: {best_linReg.degree}')\n",
    "print(f'Best lambda: {best_linReg.lambd}')\n",
    "print(f'Best L: {best_L:0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b0ab4334f2a0b1d476ecb36aa3842578",
     "grade": false,
     "grade_id": "cell-5fe08273f935ebb0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plot_test_data_fitting([best_linReg.degree], [best_linReg.lambd], fs=(12,5), show_legend=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
